{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Initial set of functions, which generate the embeddings, models and vectors as required"
      ],
      "metadata": {
        "id": "8V5_KTkpL_Gv"
      },
      "id": "8V5_KTkpL_Gv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual project ID and desired location\n",
        "project_id = \"qwiklabs-gcp-02-c706fd6470f9\"\n",
        "location = \"us\"\n",
        "connection_name = \"embedding_conn\"\n",
        "\n",
        "# Build and run the command\n",
        "!bq mk --connection \\\n",
        "  --project_id={project_id} \\\n",
        "  --location={location} \\\n",
        "  --connection_type=CLOUD_RESOURCE \\\n",
        "  {connection_name}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yL2bckqXeIA",
        "outputId": "1d520737-4ba2-4432-a1ae-86b17ebaab57"
      },
      "id": "4yL2bckqXeIA",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Already Exists: Connection\n",
            "projects/791998320083/locations/us/connections/embedding_conn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Functions"
      ],
      "metadata": {
        "id": "PPBWPQbXfsyq"
      },
      "id": "PPBWPQbXfsyq"
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import base64\n",
        "\n",
        "\n",
        "def generate_contents(chat_history):\n",
        "    \"\"\"\n",
        "    Converts a structured chat history and system instruction into Gemini-compatible contents.\n",
        "\n",
        "    Args:\n",
        "        chat_history (list): A list of dicts with keys 'role' and 'content'\n",
        "\n",
        "    Returns:\n",
        "        list[types.Content]: Gemini-compatible message sequence.\n",
        "    \"\"\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=entry[\"role\"],\n",
        "            parts=[types.Part.from_text(text=entry[\"content\"])]\n",
        "        ) for entry in chat_history\n",
        "    ]\n",
        "\n",
        "    return contents\n",
        "\n",
        "\n",
        "def generate(content, instructions,temp=0):\n",
        "  client = genai.Client(\n",
        "      vertexai=True,\n",
        "      project=\"qwiklabs-gcp-02-c706fd6470f9\",\n",
        "      location=\"us-central1\",\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  model = \"gemini-2.0-flash-001\" #Use flash because its fast\n",
        "  contents = content\n",
        "\n",
        "  generate_content_config = types.GenerateContentConfig(\n",
        "    temperature = temp, #0 Temp is better for classification\n",
        "    top_p = 1,\n",
        "    seed = 0,\n",
        "    max_output_tokens = 8192,\n",
        "    safety_settings = [types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    )],\n",
        "    system_instruction=[types.Part.from_text(text=instructions)],\n",
        "  )\n",
        "  response_text=\"\"\n",
        "  for chunk in client.models.generate_content_stream(\n",
        "    model = model,\n",
        "    contents = contents,\n",
        "    config = generate_content_config,\n",
        "    ):\n",
        "    # print(chunk.text, end=\"\") #No need to print\n",
        "    response_text += chunk.text\n",
        "  return response_text.strip()"
      ],
      "metadata": {
        "id": "BmBe349Yfrma"
      },
      "id": "BmBe349Yfrma",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding and Vector Generation"
      ],
      "metadata": {
        "id": "WYa05mgLhM9c"
      },
      "id": "WYa05mgLhM9c"
    },
    {
      "cell_type": "code",
      "id": "d2lpCjugxXW1eHMsn6uJ9YCN",
      "metadata": {
        "tags": [],
        "id": "d2lpCjugxXW1eHMsn6uJ9YCN"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "bq_client = bigquery.Client()\n",
        "\n",
        "\n",
        "# ----------- 1. Create embedding model ------------\n",
        "def create_embedding_model(model_path=\"qwiklabs-gcp-02-c706fd6470f9.jamesw_test.embeddings_model\",\n",
        "                           connection_id=\"us.embedding_conn\",\n",
        "                           endpoint=\"text-embedding-005\"):\n",
        "    \"\"\"\n",
        "    Creates a remote embedding model in BigQuery using a specified Vertex AI endpoint.\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "    CREATE OR REPLACE MODEL `{model_path}`\n",
        "    REMOTE WITH CONNECTION `{connection_id}`\n",
        "    OPTIONS (\n",
        "      ENDPOINT = '{endpoint}'\n",
        "    );\n",
        "    \"\"\"\n",
        "    bq_client.query(query).result()\n",
        "    print(f\"✅ Created embedding model: {model_path}\")\n",
        "\n",
        "\n",
        "# ----------- 2. Create embedding table ------------\n",
        "def create_qa_embedding_table(source_table, destination_table, model=\"qwiklabs-gcp-02-c706fd6470f9.jamesw_test.embeddings_model\"):\n",
        "    \"\"\"\n",
        "    Creates a new BigQuery table with embeddings of q + a using the given model.\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "  CREATE OR REPLACE TABLE `{destination_table}` AS\n",
        "  SELECT\n",
        "    *\n",
        "  FROM ML.GENERATE_EMBEDDING(\n",
        "    MODEL `{model}`,\n",
        "     (\n",
        "      SELECT\n",
        "        CONCAT(question, ' ', answer) AS content\n",
        "      FROM `{source_table}`\n",
        "    )\n",
        "  )\n",
        "  \"\"\"\n",
        "    bq_client.query(query).result()\n",
        "    print(f\"✅ Created embedding table: {destination_table}\")\n",
        "\n",
        "# ----------- 3. Embed a query and run similarity search ------------\n",
        "def run_vector_search(destination_table, query_text, model=\"qwiklabs-gcp-02-c706fd6470f9.jamesw_test.embeddings_model\", top_k=5):\n",
        "    \"\"\"\n",
        "    Embeds a query and finds top K similar rows from the embedding table using cosine similarity.\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "    CREATE TEMP TABLE input AS\n",
        "    SELECT\n",
        "      ML.GENERATE_EMBEDDING(\n",
        "        MODEL `{model}`,\n",
        "        \"{query_text}\"\n",
        "      ) AS query_emb;\n",
        "\n",
        "    SELECT\n",
        "      q, a,\n",
        "      (SELECT\n",
        "          SUM(x * y) / (SQRT(SUM(x * x)) * SQRT(SUM(y * y)))\n",
        "       FROM UNNEST(t.embedding) AS x WITH OFFSET i\n",
        "       JOIN UNNEST(input.query_emb) AS y WITH OFFSET j\n",
        "       ON i = j) AS similarity\n",
        "    FROM `{destination_table}` t, input\n",
        "    ORDER BY similarity DESC\n",
        "    LIMIT {top_k}\n",
        "    \"\"\"\n",
        "    results = bq_client.query(query).to_dataframe()\n",
        "    return results\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One time steps, generate the embedding model, the embeddings, and the vector index."
      ],
      "metadata": {
        "id": "O-R133_zJ7F7"
      },
      "id": "O-R133_zJ7F7"
    },
    {
      "cell_type": "code",
      "source": [
        "#Variables\n",
        "source_table=\"qwiklabs-gcp-02-c706fd6470f9.jamesw_test.aurora\"\n",
        "destination_table=\"qwiklabs-gcp-02-c706fd6470f9.jamesw_test.aurora_embedded\"\n",
        "index_name=\"aurora_vector_index\"\n",
        "embedding_model_path=\"qwiklabs-gcp-02-c706fd6470f9.jamesw_test.embeddings_model\"\n",
        "# 1. One-time setup\n",
        "# create_embedding_model()\n",
        "# create_qa_embedding_table(source_table,destination_table)\n",
        "\n"
      ],
      "metadata": {
        "id": "MWh3-VPYJ3Vg"
      },
      "id": "MWh3-VPYJ3Vg",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- 4. Main Search Function ------------\n",
        "def search_embedding_table(table_path, query_text, model_path, top_k=5):\n",
        "    \"\"\"\n",
        "    Embeds the input query and performs a VECTOR_SEARCH against a table of precomputed embeddings.\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "    SELECT base.content as content\n",
        "    FROM VECTOR_SEARCH(\n",
        "      TABLE `{table_path}`, 'ml_generate_embedding_result',\n",
        "      (\n",
        "        SELECT\n",
        "          ml_generate_embedding_result AS text_embedding,\n",
        "          \"{query_text}\" AS query\n",
        "        FROM ML.GENERATE_EMBEDDING(\n",
        "          MODEL `{model_path}`,\n",
        "          (SELECT \"{query_text}\" AS content)\n",
        "        )\n",
        "      ),\n",
        "      TOP_K => {top_k}\n",
        "    ) AS result\n",
        "    \"\"\"\n",
        "\n",
        "    return bq_client.query(query).to_dataframe()\n"
      ],
      "metadata": {
        "id": "6i7ew3epJKLg"
      },
      "id": "6i7ew3epJKLg",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined Functionality for Chatmode"
      ],
      "metadata": {
        "id": "rxzQmUdshbuu"
      },
      "id": "rxzQmUdshbuu"
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- 5. Chat which accounts for RAG and history ------------\n",
        "def build_chat_prompt_with_history(user_query, retrieved_df, history):\n",
        "    \"\"\"\n",
        "    Builds a multi-turn prompt using retrieved Q&A and prior user-model turns.\n",
        "    \"\"\"\n",
        "    context = \"\\n\".join([f\"QnA: {row.content}\" for _, row in retrieved_df.iterrows()])\n",
        "\n",
        "    chat = \"\\n\".join([\n",
        "        f\"User: {entry['content']}\" if entry['role'] == \"user\" else f\"Assistant: {entry['content']}\"\n",
        "        for entry in history\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "\n",
        "Below are retrieved Q&A pairs to help answer the user's question:\n",
        "\n",
        "{context}\n",
        "\n",
        "Here is the conversation so far:\n",
        "{chat}\n",
        "\n",
        "Now respond to the user's latest question:\n",
        "User: {user_query}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "# ----------- 6. Generate a chat response ------------\n",
        "def generate_chat_response(prompt_text):\n",
        "    contents = generate_contents([\n",
        "        {\"role\": \"user\", \"content\": prompt_text}\n",
        "    ])\n",
        "\n",
        "    instructions = \"\"\"You are a helpful FAQ bot for the town of Aurora Bay, Alaska. Using the user question and the FAQ context answer the question to the best of your abilities .\"\"\"\n",
        "\n",
        "    return generate(contents, instructions)\n",
        "\n",
        "# ----------- Everything combined ------------\n",
        "def multi_turn_chat_step(user_input, chat_history, table_path, index_name, model_path):\n",
        "    \"\"\"\n",
        "    Handles a single multi-turn chat step:\n",
        "    1. Vector search for relevant context\n",
        "    2. Build prompt with history\n",
        "    3. Generate model response\n",
        "    4. Append to history\n",
        "    \"\"\"\n",
        "    # Retrieve relevant Q&A\n",
        "    retrieved = search_embedding_table(table_path, user_input, model_path)\n",
        "\n",
        "    # Build full prompt with retrieved Q&A + chat history\n",
        "    full_prompt = build_chat_prompt_with_history(user_input, retrieved, chat_history)\n",
        "\n",
        "    # Run the model\n",
        "    response = generate_chat_response(full_prompt)\n",
        "\n",
        "    # Append both turns\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    chat_history.append({\"role\": \"model\", \"content\": response})\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NTxgg-P1JeDa"
      },
      "id": "NTxgg-P1JeDa",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Short Set of Examples"
      ],
      "metadata": {
        "id": "h1vRkIg7hedX"
      },
      "id": "h1vRkIg7hedX"
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    user_msg = input(\"\\nYou: \")\n",
        "    if user_msg.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    response = multi_turn_chat_step(\n",
        "        user_input=user_msg,\n",
        "        chat_history=chat_history,\n",
        "        table_path=destination_table,\n",
        "        index_name=index_name,\n",
        "        model_path=embedding_model_path\n",
        "    )\n",
        "\n",
        "    print(\"\\nAssistant:\", response)\n"
      ],
      "metadata": {
        "id": "shkleDkFJk4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b176bd7e-c06e-4e8d-a140-2cdc1a709d11"
      },
      "id": "shkleDkFJk4d",
      "execution_count": 68,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You: Test\n",
            "\n",
            "Assistant: This is a test response. I am ready to answer questions about Aurora Bay, Alaska. For example, I can tell you where the Town Hall is located, how to request a building permit, or where to find official town announcements.\n",
            "\n",
            "You: Tell me about building permits\n",
            "\n",
            "Assistant: Building permit applications can be obtained at the Town Hall’s Planning & Development Office or on the official website. A site inspection is required before approval.\n",
            "\n",
            "You: What about schools?\n",
            "\n",
            "Assistant: Aurora Bay has one elementary school, one middle school, and a combined high school. They’re managed by the Aurora Bay School District. Aurora Bay schools maintain above-average performance in statewide assessments, and the district places a strong emphasis on STEM and environmental education. There’s no university in Aurora Bay itself. The nearest college campus is located in Kodiak, roughly a 2-hour ferry trip away, depending on weather.\n",
            "\n",
            "You: exit\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-03-21d1fc6985f8 (May 12, 2025, 11:27:28 AM)",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}